{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Heykd9lIeDU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWiplXOzI5_B"
      },
      "source": [
        "Select for best 5 combinations for rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "93PuOKL8IvVs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# === Excel einlesen ===\n",
        "file_path = r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# === Zielvariable & CNN-geeignete Feature-Kandidaten ===\n",
        "target_col = \"_MKT\"\n",
        "allowed_features = [\n",
        "    \"EMP\",       # Besch√§ftigungstrend\n",
        "    \"GDP\",       # Wirtschaftswachstum\n",
        "    \"UN\",        # Arbeitslosigkeit\n",
        "    \"CPI\",       # Inflation\n",
        "    \"M2\",        # Geldmengenwachstum\n",
        "    \"Y02\",       # Kurzfristige Rendite\n",
        "    \"Y10\",       # Langfristige Rendite\n",
        "    \"STP\",       # Steilheit Zinskurve\n",
        "    \"IR\",        # Nominalzins\n",
        "    \"RR\",        # Realzins\n",
        "    \"MOV\",       # Volatilit√§t\n",
        "    \"NYF\",       # New York Fed Index\n",
        "    \"_TY\",       # Treasury Markt\n",
        "    \"_OIL\",      # √ñlpreis\n",
        "    \"_DXY\",      # Dollar Index\n",
        "    \"_LCP\",      # Large Cap Index\n",
        "    \"_AU\"        # Goldpreis\n",
        "]\n",
        "\n",
        "# === Datum verarbeiten ===\n",
        "if \"Date\" in df.columns:\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.sort_values(\"Date\")\n",
        "    df = df.set_index(\"Date\")\n",
        "\n",
        "# === Nur numerische Daten & Normalisieren ===\n",
        "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
        "\n",
        "# === Split: 15% Training, Rest Validierung ===\n",
        "split_index = int(len(df_scaled) * 0.15)\n",
        "train_df = df_scaled[:split_index]\n",
        "val_df = df_scaled[split_index:]\n",
        "\n",
        "# === Zeitreihen-Daten generieren ===\n",
        "def create_dataset(X, y, seq_len=5):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i + seq_len])\n",
        "        ys.append(y[i + seq_len])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# === RNN (LSTM) testen mit allen 3er-Kombinationen ===\n",
        "results = []\n",
        "for combo in combinations(allowed_features, 3):\n",
        "    combo = list(combo)\n",
        "    try:\n",
        "        X_train, y_train = create_dataset(train_df[combo].values, train_df[target_col].values)\n",
        "        X_val, y_val = create_dataset(val_df[combo].values, val_df[target_col].values)\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            epochs=50,\n",
        "                            batch_size=16,\n",
        "                            verbose=0,\n",
        "                            callbacks=[early_stop])\n",
        "\n",
        "        val_loss = min(history.history[\"val_loss\"])\n",
        "        results.append((combo, val_loss))\n",
        "        print(f\"‚úÖ Getestet (LSTM): {combo} | val_loss: {val_loss:.5f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Fehler bei Kombination {combo}: {str(e)}\")\n",
        "\n",
        "# === Beste 5 Kombinationen anzeigen ===\n",
        "results.sort(key=lambda x: x[1])\n",
        "print(\"\\nüèÜ Beste 5 Kombinationen mit genau 3 Features (LSTM):\")\n",
        "for i, (combo, loss) in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. {combo} ‚ûû val_loss: {loss:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "besz results  \n",
        "1. ['GDP', '_TY', '_DXY'] ‚ûû val_loss: 0.06024\n",
        "2. ['CPI', '_TY', '_LCP'] ‚ûû val_loss: 0.06063\n",
        "3. ['UN', 'Y02', '_TY'] ‚ûû val_loss: 0.06120\n",
        "4. ['_TY', '_DXY', '_LCP'] ‚ûû val_loss: 0.06159\n",
        "5. ['Y02', '_TY', '_DXY'] ‚ûû val_loss: 0.06295\n"
      ],
      "metadata": {
        "id": "cuJKDRVcRm34"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA4TsXfNI3Bg"
      },
      "source": [
        "best arhitecture for each result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "0D25t1SrJHo_",
        "outputId": "3608a9f2-3459-41a9-873b-6c7cf822f7f3"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Kopie von market_data.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3196174993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# --- Daten einlesen ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/Kopie von market_data.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# --- Datumsspalte erkennen ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Kopie von market_data.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from itertools import product\n",
        "import random\n",
        "\n",
        "# === Reproduzierbarkeit ===\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# === Daten einlesen ===\n",
        "df = pd.read_excel(r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\")\n",
        "df = df.dropna()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "target_col = \"_MKT\"\n",
        "\n",
        "# === Feature-Kombinationen ===\n",
        "combinations_to_test = [\n",
        "    ['GDP', '_TY', '_DXY'],       # ‚ûû val_loss: 0.06024\n",
        "    ['CPI', '_TY', '_LCP'],       # ‚ûû val_loss: 0.06063\n",
        "    ['UN', 'Y02', '_TY'],         # ‚ûû val_loss: 0.06120\n",
        "    ['_TY', '_DXY', '_LCP'],      # ‚ûû val_loss: 0.06159\n",
        "    ['Y02', '_TY', '_DXY']        # ‚ûû val_loss: 0.06295\n",
        "]\n",
        "\n",
        "\n",
        "# === WindowGenerator ===\n",
        "class WindowGenerator():\n",
        "    def __init__(self, input_width, label_width, shift, input_columns=None, label_columns=None, df_train=None):\n",
        "        self.label_columns = label_columns\n",
        "        self.input_columns = input_columns\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "        self.total_window_size = input_width + shift\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "\n",
        "        if df_train is not None:\n",
        "            self.train_input_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "            self.train_label_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "\n",
        "    def split_window(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.label_start:, :]\n",
        "        if self.input_columns:\n",
        "            inputs = tf.stack([inputs[:, :, self.train_input_indices[name]] for name in self.input_columns], axis=-1)\n",
        "        if self.label_columns:\n",
        "            labels = tf.stack([labels[:, :, self.train_label_indices[name]] for name in self.label_columns], axis=-1)\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, shuffle=False, batchsize=64):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            sampling_rate=1,\n",
        "            shuffle=shuffle,\n",
        "            batch_size=batchsize\n",
        "        )\n",
        "        return ds.map(self.split_window)\n",
        "\n",
        "# === Hyperparameter-Space erweitern ===\n",
        "hyperparams = list(product(\n",
        "    [10, 20, 30, 45],\n",
        "    [(32, 64, 128), (64, 64, 64), (128, 64, 32)],\n",
        "    [(0.1, 0.3), (0.2, 0.4), (0.3, 0.5)],\n",
        "    [32, 64, 128, 256]\n",
        "))\n",
        "hyperparams = random.sample(hyperparams, 40)\n",
        "\n",
        "# === Ergebnisliste ===\n",
        "final_results = []\n",
        "\n",
        "# === Loop √ºber Feature-Kombinationen ===\n",
        "for features in combinations_to_test:\n",
        "    print(f\"\\nüß™ Testing: {features}\")\n",
        "    selected_cols = features + [target_col]\n",
        "    data = df_numeric[selected_cols].copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=selected_cols)\n",
        "\n",
        "    split = int(len(data_scaled) * 0.8)\n",
        "    train_df = data_scaled[:split]\n",
        "    val_df = data_scaled[split:]\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_corr = -1\n",
        "    best_sharpe = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    for input_width, units, drops, dense in hyperparams:\n",
        "        window = WindowGenerator(input_width=input_width, label_width=1, shift=1,\n",
        "                                 input_columns=features, label_columns=[target_col], df_train=train_df)\n",
        "        train_data = window.make_dataset(train_df, shuffle=True)\n",
        "        val_data = window.make_dataset(val_df)\n",
        "\n",
        "        model = Sequential([\n",
        "            LSTM(units=units[0], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[0]),\n",
        "            LSTM(units=units[1], return_sequences=True),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[1]),\n",
        "            LSTM(units=units[2], return_sequences=False),\n",
        "            BatchNormalization(),\n",
        "            Dense(dense, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse')\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=50,\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        y_pred_val = model.predict(val_data)\n",
        "        y_true_val = np.concatenate([y for x, y in val_data], axis=0)\n",
        "\n",
        "        if y_pred_val.ndim == 3:\n",
        "            y_pred_val = y_pred_val[:, -1, :]\n",
        "        if y_true_val.ndim == 3:\n",
        "            y_true_val = y_true_val[:, -1, :]\n",
        "\n",
        "        corr, _ = pearsonr(np.ravel(y_true_val), np.ravel(y_pred_val))\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "        # Lineare Regression zur Korrektur\n",
        "        reg = LinearRegression().fit(y_pred_val.reshape(-1, 1), y_true_val.reshape(-1, 1))\n",
        "        y_pred_corrected = reg.predict(y_pred_val.reshape(-1, 1))\n",
        "\n",
        "        # Sharpe Ratio berechnen\n",
        "        returns = y_true_val[1:] - y_true_val[:-1]\n",
        "        position = np.sign(y_pred_corrected[1:] - y_pred_corrected[:-1])\n",
        "        strategy_returns = position * returns\n",
        "        sharpe_ratio = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-6)\n",
        "\n",
        "        if sharpe_ratio > best_sharpe or (sharpe_ratio == best_sharpe and corr > best_corr):\n",
        "            best_loss = val_loss\n",
        "            best_corr = corr\n",
        "            best_sharpe = sharpe_ratio\n",
        "            best_config = (input_width, units, drops, dense)\n",
        "\n",
        "    print(f\"‚úÖ Best Config: {best_config} | loss: {best_loss:.5f} | corr: {best_corr:.3f} | Sharpe: {best_sharpe:.3f}\")\n",
        "    final_results.append((features, best_loss, best_corr, best_config, best_sharpe))\n",
        "\n",
        "# === Ergebnisse anzeigen (Sharpe-basiert sortiert) ===\n",
        "final_results.sort(key=lambda x: (x[4], x[2]), reverse=True)\n",
        "print(\"\\nüèÅ Best Feature Combinations:\")\n",
        "for i, (feat, loss, corr, cfg, sharpe) in enumerate(final_results, 1):\n",
        "    print(f\"{i}. {feat} ‚ûî val_loss: {loss:.5f} | corr: {corr:.3f} | Sharpe: {sharpe:.3f} | config: {cfg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ['_TY', '_DXY', '_LCP'] ‚ûî val_loss: 0.11024 | corr: 0.694 | Sharpe: 0.127 | config: (20, (128, 64, 32), (0.1, 0.3), 128)\n",
        "2. ['CPI', '_TY', '_LCP'] ‚ûî val_loss: 0.25088 | corr: 0.876 | Sharpe: 0.126 | config: (20, (64, 64, 64), (0.2, 0.4), 256)\n",
        "3. ['GDP', '_TY', '_DXY'] ‚ûî val_loss: 0.39701 | corr: 0.347 | Sharpe: 0.099 | config: (20, (32, 64, 128), (0.2, 0.4), 256)\n",
        "4. ['UN', 'Y02', '_TY'] ‚ûî val_loss: 0.24832 | corr: 0.384 | Sharpe: 0.087 | config: (30, (128, 64, 32), (0.2, 0.4), 256)\n",
        "5. ['Y02', '_TY', '_DXY'] ‚ûî val_loss: 0.31353 | corr: 0.523 | Sharpe: 0.061 | config: (30, (128, 64, 32), (0.1, 0.3), 64)\n",
        "\n",
        "only number 1 and 2 are relevant because of sharp ratio and korelation\n",
        "\n",
        "now thind the best traiding strategy for 1 and 2"
      ],
      "metadata": {
        "id": "nbOUe6pVR_t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# === 1. Excel laden ===\n",
        "file_path = r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# === 2. Feature-Set + Parameter-Kombis definieren ===\n",
        "experiments = [\n",
        "    {\n",
        "        \"features\": ['_TY', '_DXY', '_LCP'],\n",
        "        \"lstm_units\": (128, 64, 32),\n",
        "        \"dropouts\": (0.1, 0.3),\n",
        "        \"batch_size\": 128\n",
        "    },\n",
        "    {\n",
        "        \"features\": ['CPI', '_TY', '_LCP'],\n",
        "        \"lstm_units\": (64, 64, 64),\n",
        "        \"dropouts\": (0.2, 0.4),\n",
        "        \"batch_size\": 256\n",
        "    }\n",
        "]\n",
        "\n",
        "target = '_MKT'\n",
        "seq_len = 20\n",
        "\n",
        "# === 3. Schleife √ºber Experimente ===\n",
        "for exp in experiments:\n",
        "    features = exp[\"features\"]\n",
        "    lstm_units = exp[\"lstm_units\"]\n",
        "    dropouts = exp[\"dropouts\"]\n",
        "    batch_size = exp[\"batch_size\"]\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üîç Features: {features}\")\n",
        "    print(f\"üß† Config: LSTM={lstm_units}, Dropout={dropouts}, Batch={batch_size}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # --- Daten ---\n",
        "    df_clean = df[features + [target]].dropna().reset_index(drop=True)\n",
        "    df_ret = df_clean.diff().dropna()\n",
        "    df_ret[target] = (df_ret[target] > 0).astype(int)\n",
        "\n",
        "    # --- Skalieren ---\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df_ret[features])\n",
        "    scaled_df = pd.DataFrame(scaled, columns=features)\n",
        "    scaled_df[target] = df_ret[target].values\n",
        "\n",
        "    # --- Sequenzen ---\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled_df) - seq_len):\n",
        "        X.append(scaled_df.iloc[i:i+seq_len][features].values)\n",
        "        y.append(scaled_df.iloc[i+seq_len][target])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # --- Split ---\n",
        "    split = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:split], X[split:]\n",
        "    y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "    # --- Modell ---\n",
        "    model = Sequential([\n",
        "        LSTM(lstm_units[0], return_sequences=True, input_shape=(seq_len, len(features)), kernel_regularizer=l2(0.001)),\n",
        "        Dropout(dropouts[0]),\n",
        "        LSTM(lstm_units[1], return_sequences=True, kernel_regularizer=l2(0.001)),\n",
        "        Dropout(dropouts[1]),\n",
        "        LSTM(lstm_units[2], kernel_regularizer=l2(0.001)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # --- Training ---\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=50,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # --- Loss-Plot ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f\"Train vs Validation Loss\\n{features}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss (binary_crossentropy)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Vorhersage ---\n",
        "    y_pred_proba = model.predict(X_test).flatten()\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(y_pred_proba, label=\"Predicted Signal (OOS)\", color='steelblue')\n",
        "    plt.plot(y_test, label=\"True Labels (OOS)\", color='darkorange', alpha=0.5)\n",
        "    plt.axhline(np.mean(y_pred_proba), linestyle='--', color='gray', label=\"Mean Prediction (Sharpe ref)\")\n",
        "    plt.axvline(0, linestyle=':', color='black', label=\"Train/Test Split\")\n",
        "    plt.title(f\"Prediction vs. True (Test)\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Signal-Wahrscheinlichkeit\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Strategy Return ---\n",
        "    true_returns = df_ret.iloc[-len(y_test):][target].values\n",
        "    signal = 2 * y_pred_proba - 1\n",
        "    strategy_return = signal * true_returns\n",
        "    cum_strat = np.cumsum(strategy_return)\n",
        "    cum_mkt = np.cumsum(true_returns)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(cum_strat, label=\"Strategy PnL\", linewidth=2)\n",
        "    plt.plot(cum_mkt, label=\"Market Return\", linewidth=2)\n",
        "    plt.axhline(np.mean(cum_strat), linestyle='--', color='gray', label=\"Mean Strategy PnL\")\n",
        "    plt.title(f\"Strategy Backtest\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Kumulierte Rendite\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Rolling Sharpe ---\n",
        "    rolling = pd.Series(strategy_return)\n",
        "    rolling_sharpe = rolling.rolling(20).mean() / rolling.rolling(20).std()\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(rolling_sharpe, label='Rolling Sharpe Ratio')\n",
        "    plt.axhline(rolling_sharpe.mean(), linestyle='--', color='gray', label='‚ü®Sharpe‚ü©')\n",
        "    plt.title(f\"Rolling Sharpe Ratio\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Sharpe Ratio (20d)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Metriken ---\n",
        "    ic, _ = spearmanr(y_test, y_pred_proba)\n",
        "    mean_ret = np.mean(strategy_return)\n",
        "    std_ret = np.std(strategy_return)\n",
        "    sharpe = mean_ret / std_ret if std_ret > 0 else 0\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    # --- Output ---\n",
        "    print(f\"üìä Abschluss f√ºr Features {features}\")\n",
        "    print(f\"   ‚Üí Validation Loss: {val_loss:.5f}\")\n",
        "    print(f\"   ‚Üí Information Coefficient (IC): {ic:.3f}\")\n",
        "    print(f\"   ‚Üí Sharpe Ratio: {sharpe:.3f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "2n17syymSTVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbnqySsV2tIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18/18 [==============================] - 1s 6ms/step\n",
        "üìä Abschluss f√ºr Features ['_TY', '_DXY', '_LCP']\n",
        "   ‚Üí Validation Loss: 0.70697\n",
        "   ‚Üí Information Coefficient (IC): 0.021\n",
        "   ‚Üí Sharpe Ratio: 0.718\n",
        "\n",
        "======================================================================\n",
        "üîç Features: ['CPI', '_TY', '_LCP']\n",
        "üß† Config: LSTM=(64, 64, 64), Dropout=(0.2, 0.4), Batch=256\n",
        "======================================================================\n",
        "18/18 [==============================] - 1s 5ms/step\n",
        "üìä Abschluss f√ºr Features ['CPI', '_TY', '_LCP']\n",
        "   ‚Üí Validation Loss: 0.70564\n",
        "   ‚Üí Information Coefficient (IC): 0.029\n",
        "   ‚Üí Sharpe Ratio: 0.937"
      ],
      "metadata": {
        "id": "IpeYydxY2txi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}