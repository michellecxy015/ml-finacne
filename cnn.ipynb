{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94-SvqFEBU5Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the best combination of 3 futures"
      ],
      "metadata": {
        "id": "l-7xkgszBV-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# === Excel einlesen ===\n",
        "file_path = r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# === Zielvariable & CNN-geeignete Feature-Kandidaten ===\n",
        "target_col = \"_MKT\"\n",
        "allowed_features = [\n",
        "    \"GDP\", \"UN\", \"CPI\", \"M2\",          # Makrotrends\n",
        "    \"Y02\", \"STP\", \"IR\", \"RR\",          # Kurzfristige & dynamische Zinsen\n",
        "    \"DIL\", \"MOV \", \"NYF\",               # Sentiment / Volatilit√§t\n",
        "    \"_TY\", \"_OIL\", \"_DXY\", \"_LCP\", \"_AU\"  # M√§rkte\n",
        "]\n",
        "\n",
        "# === Datum verarbeiten ===\n",
        "if \"Date\" in df.columns:\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.sort_values(\"Date\")\n",
        "    df = df.set_index(\"Date\")\n",
        "\n",
        "# === Nur numerische Daten & Normalisieren ===\n",
        "df = df.select_dtypes(include=[\"number\"]).dropna()\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
        "\n",
        "# === Split: 15% Training, Rest Validierung ===\n",
        "split_index = int(len(df_scaled) * 0.15)\n",
        "train_df = df_scaled[:split_index]\n",
        "val_df = df_scaled[split_index:]\n",
        "\n",
        "# === Zeitreihen-Daten generieren ===\n",
        "def create_dataset(X, y, seq_len=5):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i + seq_len])\n",
        "        ys.append(y[i + seq_len])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# === CNN testen mit allen 3er-Kombinationen ===\n",
        "results = []\n",
        "for combo in combinations(allowed_features, 3):\n",
        "    combo = list(combo)\n",
        "    try:\n",
        "        X_train, y_train = create_dataset(train_df[combo].values, train_df[target_col].values)\n",
        "        X_val, y_val = create_dataset(val_df[combo].values, val_df[target_col].values)\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(32, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            epochs=50,\n",
        "                            batch_size=16,\n",
        "                            verbose=0,\n",
        "                            callbacks=[early_stop])\n",
        "\n",
        "        val_loss = min(history.history[\"val_loss\"])\n",
        "        results.append((combo, val_loss))\n",
        "        print(f\"‚úÖ Getestet: {combo} | val_loss: {val_loss:.5f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Fehler bei Kombination {combo}: {str(e)}\")\n",
        "\n",
        "# === Beste 5 Kombinationen anzeigen ===\n",
        "results.sort(key=lambda x: x[1])\n",
        "print(\"\\nüèÜ Beste 5 Kombinationen mit genau 3 Features:\")\n",
        "for i, (combo, loss) in enumerate(results[:5], 1):\n",
        "    print(f\"{i}. {combo} ‚ûû val_loss: {loss:.5f}\")"
      ],
      "metadata": {
        "id": "8WEpE-MNBZ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ['Y02', '_TY', '_AU'] ‚ûû val_loss: 0.01596\n",
        "2. ['GDP', '_TY', '_DXY'] ‚ûû val_loss: 0.03067\n",
        "3. ['IR', '_TY', '_LCP'] ‚ûû val_loss: 0.03293\n",
        "4. ['UN', '_TY', '_OIL'] ‚ûû val_loss: 0.03300\n",
        "5. ['Y02', 'IR', '_TY'] ‚ûû val_loss: 0.03404\n",
        "\n",
        "\n",
        "thind the best configuration  for each future combination\n"
      ],
      "metadata": {
        "id": "4rz3PjuIBesU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import pearsonr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, GlobalAveragePooling1D, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "import random\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# === Fixe Seeds f√ºr Reproduzierbarkeit ===\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# === Excel-Daten laden ===\n",
        "df = pd.read_excel(r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\")\n",
        "df = df.dropna()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# === Nur numerische Spalten + Ziel definieren ===\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "target_col = \"_MKT\"\n",
        "\n",
        "# === Feature-Kombinationen zum Testen ===\n",
        "combinations_to_test = [\n",
        "    ['Y02', '_TY', '_AU'],\n",
        "    ['GDP', '_TY', '_DXY'],\n",
        "    ['IR', '_TY', '_LCP'],\n",
        "    ['UN', '_TY', '_OIL'],\n",
        "    ['Y02', 'IR', '_TY']\n",
        "]\n",
        "\n",
        "# === WindowGenerator-Klasse f√ºr Zeitfenster ===\n",
        "class WindowGenerator():\n",
        "    def __init__(self, input_width, label_width, shift, input_columns=None, label_columns=None, df_train=None):\n",
        "        self.label_columns = label_columns\n",
        "        self.input_columns = input_columns\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "        self.total_window_size = input_width + shift\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "\n",
        "        if df_train is not None:\n",
        "            self.train_input_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "            self.train_label_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
        "\n",
        "    def split_window(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.label_start:, :]\n",
        "        if self.input_columns:\n",
        "            inputs = tf.stack([inputs[:, :, self.train_input_indices[name]] for name in self.input_columns], axis=-1)\n",
        "        if self.label_columns:\n",
        "            labels = tf.stack([labels[:, :, self.train_label_indices[name]] for name in self.label_columns], axis=-1)\n",
        "        return inputs, labels\n",
        "\n",
        "    def make_dataset(self, data, shuffle=False, batchsize=64):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            sampling_rate=1,\n",
        "            shuffle=shuffle,\n",
        "            batch_size=batchsize\n",
        "        )\n",
        "        return ds.map(self.split_window)\n",
        "\n",
        "# === Hyperparameter-Raum ===\n",
        "hyperparams = list(product(\n",
        "    [10, 20, 30, 45],                    # input window sizes\n",
        "    [(2, 3, 5), (3, 5, 7), (2, 4, 6)],   # kernel sizes per layer\n",
        "    [(32, 64, 128)],                    # filters per Conv1D layer\n",
        "    [(0.1, 0.3), (0.2, 0.4)],           # dropout values\n",
        "    [32, 64, 128]                  # number of neurons in dense layer\n",
        "))\n",
        "hyperparams = random.sample(hyperparams, 30)\n",
        "\n",
        "# === Ergebnisse speichern ===\n",
        "final_results = []\n",
        "\n",
        "# === Modelle pro Feature-Kombination testen ===\n",
        "for features in combinations_to_test:\n",
        "    print(f\"\\nüß™ Testing combination: {features}\")\n",
        "    selected_cols = features + [target_col]\n",
        "    data = df_numeric[selected_cols].copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=selected_cols)\n",
        "\n",
        "    split = int(len(data_scaled) * 0.8)\n",
        "    train_df = data_scaled[:split]\n",
        "    val_df = data_scaled[split:]\n",
        "\n",
        "    best_loss = np.inf\n",
        "    best_model = None\n",
        "    best_config = None\n",
        "    best_corr = -1\n",
        "\n",
        "    for input_width, ksizes, filters, drops, dense in hyperparams:\n",
        "        window = WindowGenerator(input_width=input_width, label_width=1, shift=1,\n",
        "                                 input_columns=features, label_columns=[target_col], df_train=train_df)\n",
        "        train_data = window.make_dataset(train_df, shuffle=True)\n",
        "        val_data = window.make_dataset(val_df)\n",
        "\n",
        "        model = Sequential([\n",
        "            Conv1D(filters[0], kernel_size=ksizes[0], activation='relu', padding='causal'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[0]),\n",
        "\n",
        "            Conv1D(filters[1], kernel_size=ksizes[1], activation='relu', padding='causal'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(drops[1]),\n",
        "\n",
        "            Conv1D(filters[2], kernel_size=ksizes[2], activation='relu', padding='causal'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            GlobalAveragePooling1D(),\n",
        "            Dense(dense, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse')\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=50,\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        y_pred_val = model.predict(val_data)\n",
        "        y_true_val = np.concatenate([y for x, y in val_data], axis=0)\n",
        "\n",
        "        if y_pred_val.ndim == 3:\n",
        "            y_pred_val = y_pred_val[:, -1, :]\n",
        "        if y_true_val.ndim == 3:\n",
        "            y_true_val = y_true_val[:, -1, :]\n",
        "\n",
        "        corr, _ = pearsonr(np.ravel(y_true_val), np.ravel(y_pred_val))\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "        if corr > best_corr or (corr == best_corr and val_loss < best_loss):\n",
        "            best_loss = val_loss\n",
        "            best_model = model\n",
        "            best_config = (input_width, ksizes, filters, drops, dense)\n",
        "            best_corr = corr\n",
        "\n",
        "    print(f\"‚úÖ Best Config: {best_config} | val_loss: {best_loss:.5f} | corr: {best_corr:.3f}\")\n",
        "    final_results.append((features, best_loss, best_corr, best_config))\n",
        "\n",
        "# === Ergebnisse sortiert nach Korrelation anzeigen ===\n",
        "final_results.sort(key=lambda x: -x[2])"
      ],
      "metadata": {
        "id": "c_tX_-SnBpVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing combination: ['Y02', '_TY', '_AU']\n",
        "Best Config: (10, (3, 5, 7), (32, 64, 128), (0.2, 0.4), 128) | val_loss: 0.21160 | corr: 0.855\n",
        "Testing combination: ['GDP', '_TY', '_DXY']\n",
        "Best Config: (45, (3, 5, 7), (32, 64, 128), (0.1, 0.3), 128) | val_loss: 0.24084 | corr: 0.535\n",
        "Testing combination: ['IR', '_TY', '_LCP']\n",
        "Best Config: (45, (3, 5, 7), (32, 64, 128), (0.2, 0.4), 128) | val_loss: 0.18293 | corr: 0.851\n",
        "Testing combination: ['UN', '_TY', '_OIL']\n",
        "Best Config: (30, (2, 4, 6), (32, 64, 128), (0.2, 0.4), 128) | val_loss: 0.32756 | corr: 0.747\n",
        "Testing combination: ['Y02', 'IR', '_TY']\n",
        "Best Config: (45, (3, 5, 7), (32, 64, 128), (0.2, 0.4), 64) | val_loss: 0.37596 | corr: 0.872\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fi5Y40I5ByID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we seleckt only  ['Y02', '_TY', '_AU']  and ['UN', '_TY', '_OIL'] because 2 best ones\n"
      ],
      "metadata": {
        "id": "tELDxvgdCBBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# === Excel-Datei laden ===\n",
        "file_path = r\"C:\\Users\\41799\\Desktop\\Kopie von market_data.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# === Konfigurationen ===\n",
        "experiments = [\n",
        "    {\n",
        "        \"features\": ['Y02', '_TY', '_AU'],\n",
        "        \"config\": (10, (3, 5, 7), (32, 64, 128), (0.2, 0.4), 128)\n",
        "    },\n",
        "    {\n",
        "        \"features\": ['UN', '_TY', '_OIL'],\n",
        "        \"config\": (45, (3, 5, 7), (32, 64, 128), (0.2, 0.4), 128)\n",
        "    }\n",
        "]\n",
        "\n",
        "target = '_MKT'\n",
        "\n",
        "for exp in experiments:\n",
        "    features = exp[\"features\"]\n",
        "    seq_len, kernel_sizes, filters, dropouts, batch_size = exp[\"config\"]\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"üîç Features: {features}\")\n",
        "    print(f\"üß† CNN Config: Kernels={kernel_sizes}, Filters={filters}, Dropouts={dropouts}, Seq={seq_len}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- Daten vorbereiten ---\n",
        "    df_clean = df[features + [target]].dropna().reset_index(drop=True)\n",
        "    df_ret = df_clean.diff().dropna()\n",
        "    df_ret[target] = (df_ret[target] > 0).astype(int)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df_ret[features])\n",
        "    scaled_df = pd.DataFrame(scaled, columns=features)\n",
        "    scaled_df[target] = df_ret[target].values\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled_df) - seq_len):\n",
        "        X.append(scaled_df.iloc[i:i + seq_len][features].values)\n",
        "        y.append(scaled_df.iloc[i + seq_len][target])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # --- Split ---\n",
        "    split = int(len(X) * 0.7)\n",
        "    X_train, X_test = X[:split], X[split:]\n",
        "    y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "    # --- Modell ---\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=filters[0], kernel_size=kernel_sizes[0], padding='same', activation='relu',\n",
        "                     input_shape=(seq_len, len(features)), kernel_regularizer=l2(0.003)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(dropouts[0]))\n",
        "    model.add(Conv1D(filters=filters[1], kernel_size=kernel_sizes[1], padding='same', activation='relu',\n",
        "                     kernel_regularizer=l2(0.003)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(dropouts[1]))\n",
        "    model.add(Conv1D(filters=filters[2], kernel_size=kernel_sizes[2], padding='same', activation='relu',\n",
        "                     kernel_regularizer=l2(0.003)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=50,\n",
        "                        batch_size=batch_size, callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    y_pred_proba = model.predict(X_test).flatten()\n",
        "    signal = 2 * y_pred_proba - 1\n",
        "    true_returns = df_ret.iloc[-len(y_test):][target].values\n",
        "    strategy_return = signal * true_returns\n",
        "\n",
        "    ic, _ = spearmanr(y_test, y_pred_proba)\n",
        "    mean_ret = np.mean(strategy_return)\n",
        "    std_ret = np.std(strategy_return)\n",
        "    sharpe = mean_ret / std_ret if std_ret > 0 else 0\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    print(f\"‚úÖ Done for {features}\")\n",
        "    print(f\"Sharpe Ratio: {sharpe:.3f} | Expected Return: {mean_ret:.4f} | Volatility: {std_ret:.4f}\")\n",
        "    print(f\"Corr(raw): {ic:.3f}\")\n",
        "    print(f\"Return Series Sample: {strategy_return[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # --- PLOTS ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f\"Train vs Validation Loss\\n{features}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss (binary_crossentropy)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(y_pred_proba, label=\"Predicted Signal\", color='blue')\n",
        "    plt.plot(y_test, label=\"True Label\", alpha=0.5, color='orange')\n",
        "    plt.axhline(np.mean(y_pred_proba), linestyle='--', color='gray', label=\"Mean Prediction\")\n",
        "    plt.title(f\"Prediction vs. True (Test)\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Signal-Wahrscheinlichkeit\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    cum_strat = np.cumsum(strategy_return)\n",
        "    cum_mkt = np.cumsum(true_returns)\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(cum_strat, label=\"Strategy PnL\", linewidth=2)\n",
        "    plt.plot(cum_mkt, label=\"Market Return\", linewidth=2)\n",
        "    plt.axhline(np.mean(cum_strat), linestyle='--', color='gray', label=\"Mean Strategy PnL\")\n",
        "    plt.title(f\"Strategy Backtest\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Kumulierte Rendite\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    rolling = pd.Series(strategy_return)\n",
        "    rolling_sharpe = rolling.rolling(20).mean() / rolling.rolling(20).std()\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(rolling_sharpe, label='Rolling Sharpe Ratio')\n",
        "    plt.axhline(rolling_sharpe.mean(), linestyle='--', color='gray', label='‚ü®Sharpe‚ü©')\n",
        "    plt.title(f\"Rolling Sharpe Ratio\\n{features}\")\n",
        "    plt.xlabel(\"Zeitindex\")\n",
        "    plt.ylabel(\"Sharpe Ratio (20d)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qa16IzKKCZzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p63kUWMuDRa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Done for ['Y02', '_TY', '_AU']\n",
        "Sharpe Ratio: 0.635 | Expected Return: 0.0578 | Volatility: 0.0911\n",
        "Corr(raw): 0.029\n",
        "Return Series Sample: [-0.03940409  0.          0.         -0.0462988   0.        ]\n",
        "\n",
        "======================================================================\n",
        "üîç Features: ['UN', '_TY', '_OIL']\n",
        "üß† CNN Config: Kernels=(3, 5, 7), Filters=(32, 64, 128), Dropouts=(0.2, 0.4), Seq=45\n",
        "======================================================================\n",
        "18/18 [==============================] - 0s 2ms/step\n",
        "‚úÖ Done for ['UN', '_TY', '_OIL']\n",
        "Sharpe Ratio: 0.536 | Expected Return: 0.1017 | Volatility: 0.1897\n",
        "Corr(raw): -0.038\n",
        "Return Series Sample: [0.00513327 0.02170122 0.0868088  0.         0.12348986]\n"
      ],
      "metadata": {
        "id": "XBRNyFAquENF"
      }
    }
  ]
}